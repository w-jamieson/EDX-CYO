---
title: "Choose Your Own Project - Video Game Sales with Ratings"
author: "Wayne Jamieson"
output: pdf_document
date: "2022-10-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
For the Data Science: Capstone 'Choose Your Own' project I have considered a Video Game Sales with Ratings dataset. This dataset was made available via kaggle.com and consists of a web scrape of VGChartz Video Game Sales data extended with critic and user score data from Meta Critic. I have considered a number of predictive approaches using machine learning in relation to expected sales along with critic and user reception.

The approach taken can be summarised as:

- Download and create the game sales dataset
- Analyse the dataset to understand its variables and properties
- Transform the sales data for predictive analysis
- Create train and test sets from game sales dataset
- Conduct analysis of algorithmic and logisitic models to predict game sales
- Perform further transformations to exclude missing score data in a separate dataset
- Consider linear regression models for the relationships between critic & user scores in determining game sales
- Conduct analysis of algorithmic models to predict critical and user reception

Start by installing and loading the required libraries then downloading from a csv in a github repository and creating the game sales dataset.


```{r create, message = FALSE, warning = FALSE}
# Install if required and load packages
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(arules)) install.packages("arules", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(randomForest)
library(arules)

# Download and read the vieao game sales csv file
dl <- tempfile()
download.file("https://github.com/w-jamieson/EDX-CYO/raw/main/Video_Games_Sales_as_at_22_Dec_2016.csv", dl)

gamesales <- read.csv(file = dl)
```

## Methods & Analysis

We start by examing the game sales dataset created.

```{r effect, warning = FALSE, echo=FALSE}
str(gamesales)
head(gamesales)
```

The data has 16,719 unique games and  consists of 16 variables capturing:

- Identification and classification data such as:
  + Name 
  + Platform
  + Year of Release
  + Genre
  + Publisher
  + Developer
  + Rating
- Sales figures in millions by region
- Critic scores and number of reviews
- User Scores and number of reviews

It is noted that critic and user scores are not available for all games and user scores are returned as a character variable. These points will need to be adjusted via transformation prior to conducting analysis related to critic and user scores.

We next look at high level characteristics of the Global Sales data

```{r sales, warning = FALSE}
# Number of games
nrow(gamesales)
# Total sales
sum(gamesales$Global_Sales)
# Average sales
mean(gamesales$Global_Sales)
# Standard deviation of sales
sd(gamesales$Global_Sales)
# Highest selling game figure
max(gamesales$Global_Sales)
```

Its noted that a small number of the many games make up the bulk of the sales.

We continue by considering analysis of sales data by various characteristics, firstly by Genre looking at:

- Number of titles
- Total sales
- Average sales per title
- Standard deviation of sales

```{r sales by genre, warning=FALSE, echo=FALSE}
# Games by genre
# Number of titles
ggplot(gamesales, aes(x = Genre)) +
  geom_bar() + ggtitle("Number of Titles")
gamesales %>%
  count(Genre)
# Total sales
ggplot(gamesales, aes(x = Genre, y = Global_Sales)) + geom_bar(stat = "identity") + ggtitle("Total Sales")
gamesales %>% 
  group_by(Genre) %>% 
  summarise(TotalSales = sum(Global_Sales))
# Average sales
ggplot(gamesales, aes(x = Genre, y = Global_Sales)) + stat_summary(fun="mean", geom = "bar") + ggtitle("Average Sales")
gamesales %>% 
  group_by(Genre) %>% 
  summarise(AvgSales = mean(Global_Sales))
# Standard deviation of sales
ggplot(gamesales, aes(x = Genre, y = Global_Sales)) + stat_summary(fun="sd", geom = "bar") + ggtitle("Standard Deviation of Sales")
gamesales %>% 
  group_by(Genre) %>% 
  summarise(SdSales = sd(Global_Sales))
# Summary
gamesales %>% 
  group_by(Genre) %>% 
  summarise(TotalSales = sum(Global_Sales), AvgSales = mean(Global_Sales), SdSales = sd(Global_Sales)) %>%
  arrange(., desc(TotalSales))

```

Repeating the above analysis by Platform:

```{r sales by platform, warning=FALSE, echo=FALSE}
# Games by platform
# Number of titles
ggplot(gamesales, aes(x = Platform)) +
  geom_bar() + ggtitle("Number of Titles")
gamesales %>%
  count(Platform)
# Total sales
ggplot(gamesales, aes(x = Platform, y = Global_Sales)) + geom_bar(stat = "identity") + ggtitle("Total Sales")
gamesales %>% 
  group_by(Platform) %>% 
  summarise(TotalSales = sum(Global_Sales))
# Average sales
ggplot(gamesales, aes(x = Platform, y = Global_Sales)) + stat_summary(fun="mean", geom = "bar") + ggtitle("Average Sales")
gamesales %>% 
  group_by(Platform) %>% 
  summarise(AvgSales = mean(Global_Sales))
# Standard deviation of sales
ggplot(gamesales, aes(x = Platform, y = Global_Sales)) + stat_summary(fun="sd", geom = "bar")  + ggtitle("Standard Deviation of Sales")
gamesales %>% 
  group_by(Platform) %>% 
  summarise(SdSales = sd(Global_Sales))
#Summary
gamesales %>% 
  group_by(Platform) %>% 
  summarise(TotalSales = sum(Global_Sales), AvgSales = mean(Global_Sales), SdSales = sd(Global_Sales)) %>%
  arrange(., desc(TotalSales))

```

Repeating the analysis by Publisher (tables outputs heavily summarised due to number of publishers):

```{r sales by publisher, warning=FALSE, echo=FALSE}
# Games by publisher
# Number of titles
ggplot(gamesales, aes(x = Publisher)) +
  geom_bar() + ggtitle("Number of Titles")
head(gamesales %>%
  count(Publisher))
# Total sales
ggplot(gamesales, aes(x = Publisher, y = Global_Sales)) + geom_bar(stat = "identity") + ggtitle("Total Sales")
gamesales %>% 
  group_by(Publisher) %>% 
  summarise(TotalSales = sum(Global_Sales))
# Average sales
ggplot(gamesales, aes(x = Publisher, y = Global_Sales)) + stat_summary(fun="mean", geom = "bar") + ggtitle("Average Sales")
gamesales %>% 
  group_by(Publisher) %>% 
  summarise(AvgSales = mean(Global_Sales))
# Standard deviation of sales
ggplot(gamesales, aes(x = Publisher, y = Global_Sales)) + stat_summary(fun="sd", geom = "bar") + ggtitle("Standard Deviation of Sales")
gamesales %>% 
  group_by(Publisher) %>% 
  summarise(SdSales = sd(Global_Sales))
#Summary
gamesales %>% 
  group_by(Publisher) %>% 
  summarise(TotalSales = sum(Global_Sales), AvgSales = mean(Global_Sales), SdSales = sd(Global_Sales)) %>%
  arrange(., desc(TotalSales))
```

Finally repeat the analysis by Year:

```{r sales by year, warning=FALSE, echo=FALSE}
# Games by year
# Number of titles
ggplot(gamesales, aes(x = Year_of_Release)) +
  geom_bar() + ggtitle("Number of Titles")
gamesales %>%
  count(Year_of_Release)
# Total sales
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales)) + geom_bar(stat = "identity") + ggtitle("Total Sales")
gamesales %>% 
  group_by(Year_of_Release) %>% 
  summarise(TotalSales = sum(Global_Sales))
# Average sales
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales)) + stat_summary(fun="mean", geom = "bar") + ggtitle("Average Sales")
gamesales %>% 
  group_by(Year_of_Release) %>% 
  summarise(AvgSales = mean(Global_Sales))
# Standard deviation of sales
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales)) + stat_summary(fun="sd", geom = "bar") + ggtitle("Standard Deviation of Sales")
gamesales %>% 
  group_by(Year_of_Release) %>% 
  summarise(SdSales = sd(Global_Sales))
#Summary
gamesales %>% 
  group_by(Year_of_Release) %>% 
  summarise(TotalSales = sum(Global_Sales), AvgSales = mean(Global_Sales), SdSales = sd(Global_Sales)) %>%
  arrange(., desc(TotalSales))
```

The year with the most total sales is 2008.

We can also break up the sales per year by genre & platform (there are too many publishers to chart by this characteristic) 

```{r sales by year vs genre_platform, warning=FALSE, echo=FALSE}
# Yearly game sales by genre
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales, fill = Genre)) + geom_bar(stat = "identity") + ggtitle("Yearly Sales by Genre")
# Yearly game sales by platform
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales, fill = Platform)) + geom_bar(stat = "identity") + ggtitle("Yearly Sales by Platform")
```

From the prior analysis by year it can be seen that sales data prior to 1996 is highly statistically different to future years. Also data appears incomplete post 2016. We now filter the dataset to years between these values.

```{r filter years, warning=FALSE}
#Sales data prior to 1996 is highly statistically different to future years. Also data appears incomplete post 2016. Filtering to years between these values.
gamesales <- gamesales %>% 
  filter(Year_of_Release >= 1996) %>%
  filter(Year_of_Release < 2016)
```

```{r filter years chart, warning=FALSE,echo=FALSE}
ggplot(gamesales, aes(x = Year_of_Release, y = Global_Sales)) + geom_bar(stat = "identity") + ggtitle("Total Sales")
```

Also Wii Sports can be seen as an extreme outlier relative to other sales. We remove this title as it is unlikely to support predictive modelling.

```{r filter wii sports, warning=FALSE}
# Also Wii Sports can be seen as extreme outlier relative to other sales. Removing this title as it as unlikely to support predictive modelling.
gamesales <- gamesales %>% 
    filter(Name != "Wii Sports") 
```

Another observation is that a large number of titles are missing a Developer. Filtering out titles that do not have a developer such that this characteristic can also be used in modelling.

```{r filter developer, warning=FALSE}
# Filter out missing developers
gamesales <- gamesales %>% 
  filter(Developer != "") 
head(gamesales)
```

Next we consider the distribution of sales by region and globally.

```{r sales distribution, warning=FALSE, echo=FALSE}
# Distribution of sales data
gamesales %>%
  ggplot(aes(NA_Sales)) + geom_histogram(binwidth = 1) + ggtitle("North America")
gamesales %>%
  ggplot(aes(JP_Sales)) + geom_histogram(binwidth = 1) + ggtitle("Japan")
gamesales %>%
  ggplot(aes(EU_Sales)) + geom_histogram(binwidth = 1) + ggtitle("Europe")
gamesales %>%
  ggplot(aes(Other_Sales)) + geom_histogram(binwidth = 1) + ggtitle("Other")
gamesales %>%
  ggplot(aes(Global_Sales)) + geom_histogram(binwidth = 1) + ggtitle("Global")
```

Distribution of sales data does not appear to be normal with a very small proportion of games making up the bulk of the sales. Adding a variable to the dataset converting Global Sales to log normal for future regression analysis.

```{r global log, warning=FALSE}
# Converting sales to log to create normal distribution assumption
gamesales <- gamesales %>%
  mutate(Global_Sales_log = log(Global_Sales))
gamesales %>%
    ggplot(aes(Global_Sales_log)) + geom_histogram(binwidth = 1) + ggtitle("Global Sales log normal")
```

In future analysis we will consider Random Forest as a logistic prediction model. Creating ranged buckets with even observations of global sales to perform this analysis.

```{r global categories, warning=FALSE}
# Add categorical bins of global for classification prediction models
gamesales <- gamesales %>%
  mutate(Global_Sales_range = discretize(Global_Sales, method = "frequency", breaks = 10))
head(gamesales)
table(gamesales$Global_Sales_range)
```

We can now create train and tests on the game sales data including removing missing genres, platforms, publishers and developers from both sets.

```{r train_test, warning=FALSE}
#create train & test sets for data
set.seed(1,sample.kind="Rounding")
test_index <- createDataPartition(y = gamesales$Global_Sales_log, times = 1, p = 0.2, list = FALSE) 
train_set <- gamesales[-test_index,]
test_set <- gamesales[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "Genre") %>%
  semi_join(train_set, by = "Platform") %>%
  semi_join(train_set, by = "Publisher") %>%
  semi_join(train_set, by = "Developer")
```

The first set of analysis will consider an algorithm to predict expected global sales based on effects including genre, platform, publisher & developer. Results will be assessed by reducing the Root Mean Squared Error (RMSE). Start by creating a function to calculate the RMSE.

```{r RMSE, warning=FALSE}
# Function to calculate RMSE
RMSE <- function(true_sales, predicted_sales){
  sqrt(mean((true_sales - predicted_sales)^2))
}
```

Next we'll consider a basic model assuming the average of all sales as the predicted result.

```{r sales_naive_average, warning=FALSE}
# Model assuming same sames for all games
# Average
mu_hat <- mean(train_set$Global_Sales)
mu_hat 

```

```{r sales_naive_RMSE, warning=FALSE, echo=FALSE}
naive_rmse <- RMSE(test_set$Global_Sales, mu_hat)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

rmse_results %>% knitr::kable()
```

Next we look at whether the RMSE can be improved by adjusting by the average of different characteristic effects starting with Genre.

```{r sales_g, warning=FALSE, echo=FALSE}
mu <- mean(train_set$Global_Sales) 
genre_avgs <- train_set %>% 
  group_by(Genre) %>% 
  summarize(b_g = mean(Global_Sales - mu))

predicted_sales <- mu + test_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  .$b_g

model_1_rmse <- RMSE (test_set$Global_Sales,predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre Effect Model",
                                     RMSE = model_1_rmse))

rmse_results %>% knitr::kable()
```

Continue by incrementally adding average effects for Platform, Publisher & Developer to model

```{r sales_g_p_u_d, warning=FALSE, echo=FALSE}
# Model based on genre and platform effect
platform_avgs <- train_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  group_by(Platform) %>%
  summarize(b_p = mean(Global_Sales - mu - b_g))

predicted_sales <- test_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  mutate(pred = mu + b_g + b_p) %>%
  .$pred

model_2_rmse <- RMSE (test_set$Global_Sales, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform Effects Model",  
                                     RMSE = model_2_rmse ))

# Model based on genre, platform & publisher effect
publisher_avgs <- train_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  group_by(Publisher) %>%
  summarize(b_u = mean(Global_Sales - mu - b_g - b_p))

predicted_sales <- test_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  mutate(pred = mu + b_g + b_p + b_u) %>%
  .$pred

model_3_rmse <- RMSE (test_set$Global_Sales, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher Effects Model",  
                                     RMSE = model_3_rmse ))

# Model based on genre, platform, publisher & developer effect
developer_avgs <- train_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  group_by(Developer) %>%
  summarize(b_d = mean(Global_Sales - mu - b_g - b_p - b_u))

predicted_sales <- test_set %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  left_join(developer_avgs, by='Developer') %>%
  mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
  .$pred

model_4_rmse <- RMSE (test_set$Global_Sales, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher + Developer Effects Model",  
                                     RMSE = model_4_rmse ))
rmse_results %>% knitr::kable()
```

The RMSE continues to improve with each effect, however, considering of starting average of 0.60 an RMSE of 1.373 does not reflect a highly predictive model. Will also consider regularization impact on low observations by optimising for the penalty terms on all effects.

```{r regularisation, warning=FALSE,echo=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$Global_Sales)
  b_g <- train_set %>%
    group_by(Genre) %>%
    summarize(b_g = sum(Global_Sales - mu)/(n()+l))
  b_p <- train_set %>% 
    left_join(b_g, by='Genre') %>%
    group_by(Platform) %>%
    summarize(b_p = sum(Global_Sales - b_g - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    group_by(Publisher) %>%
    summarize(b_u = sum(Global_Sales - b_g - b_p - mu)/(n()+l))
  b_d <- train_set %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>%
    group_by(Developer) %>%
    summarize(b_d = sum(Global_Sales - b_g - b_p - b_u - mu)/(n()+l))
  predicted_sales <- 
    test_set %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>% 
    left_join(b_d, by='Developer') %>% 
    mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
    .$pred
  return(RMSE(test_set$Global_Sales, predicted_sales))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Genre + Platform + Publisher + Developer Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

The optimal lambda is 0.75 however there is no material improvement to the RMSE.

Will now consider random forest as a logistic prediction model of the global sales ranges determined above based on the same predictors as the previous algorithm.

```{r rf, warning=FALSE, echo=FALSE}
# Testing using categorical predictor model such as Random Forest
train_rf <- randomForest(Global_Sales_range ~ Genre + Platform + Publisher, data = train_set)
preds_rf <- predict(train_rf,test_set)
confusionMatrix(preds_rf,test_set$Global_Sales_range)$overall["Accuracy"]
```

The accuracy of this model is very low at 14%. Limiting the predictors to just Genre & Platfrom improves the accuracy slightly.

```{r rf g_p, warning=FALSE, echo=FALSE}
# Accuracy very low. Model using only Genre & Platform slightly higher
train_rf <- randomForest(Global_Sales_range ~ Genre + Platform, data = train_set)
preds_rf <- predict(train_rf,test_set)
confusionMatrix(preds_rf,test_set$Global_Sales_range)$overall["Accuracy"]
```

However, at just 18% accuracy this is still very low and it appears these models will not be useful in attempting to predict continuous sales data.

We continue with further analyses utilising critic and user scores. To undertake this analysis we will first cleanse the data by filtering out all games that are missing critic and user scores in a new dataset.

```{r filter scores, warning=FALSE, echo=FALSE}
#Filter out NA Critic & User Scores
gamesales_critic <- gamesales %>%
  filter(Critic_Count != 'NA') %>%
  filter(User_Count != 'NA')
head(gamesales_critic)
```

To achieve greater consistency across all the scoring data will convert character user scores to numeric and all scores to a 5 point scale.

```{r cleanse scores, warning=FALSE, echo=FALSE}
# Convert user scores to numeric and also convert all scores to 5 point scale
gamesales_critic <- gamesales_critic %>%
  mutate(Critic_Score = Critic_Score / 20) %>%
  mutate(User_Score = as.numeric(User_Score)/2) 
head(gamesales_critic)
```

To start this next stage of analysis we will first look at the distribution of critic & user scores.

```{r score distributions, warning=FALSE,echo=FALSE}
#Distribution of critic scores
gamesales_critic %>%
  ggplot(aes(Critic_Score)) + geom_histogram(binwidth = 0.5) + ggtitle("Critic Score Distribution")

#Distribution of user scores
gamesales_critic %>%
  ggplot(aes(User_Score)) + geom_histogram(binwidth = 0.5) + ggtitle("User Score Distribution")
```

Both critic & user scores appear to be normally distributed.

Next we look at the relationship between Critic/User Scores/Count and Global Sales (log normal) 

```{r score vs sales, warning=FALSE,echo=FALSE, message=FALSE}
#Investigate relationship between critic & user scores and global sales
ggplot(gamesales_critic, aes(x = Critic_Score, y = Global_Sales_log)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Global Sales (log) vs Critic Score")
ggplot(gamesales_critic, aes(x = User_Score, y = Global_Sales_log)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Global Sales (log) vs User Score")

ggplot(gamesales_critic, aes(x = Critic_Count, y = Global_Sales_log)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Global Sales (log) vs Critic Count")
ggplot(gamesales_critic, aes(x = User_Count, y = Global_Sales_log)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Global Sales (log) vs User Count")
```

There is a clear linear relationship between scoring metrics and global sales with the relationship least strong with User Count.

We will now create train and test sets based on the scoring cleansed dataset.

```{r train critic, warning=FALSE}
#create train & test sets for critic filtered data
set.seed(1,sample.kind="Rounding")
test_index <- createDataPartition(y = gamesales_critic$Global_Sales, times = 1, p = 0.2, list = FALSE) 
train_critic <- gamesales_critic[-test_index,]
test_critic <- gamesales_critic[test_index,]

test_critic <- test_critic %>% 
  semi_join(train_critic, by = "Genre") %>% 
  semi_join(train_critic, by = "Platform") %>%
  semi_join(train_critic, by = "Publisher") %>%
  semi_join(train_critic, by = "Developer")

```

First consider mean squared loss of assuming the average of all sales followed linear regression models on each of the 4 individual scoring metrics.

```{r linear individual, warning=FALSE, echo=FALSE}
# guess as average of global sales
m <- mean(train_critic$Global_Sales_log)
naive_sl <- mean((m-test_critic$Global_Sales_log)^2)

SL_results <- data_frame(method = "Just the average", SL = naive_sl)

# fit linear regression model on global_sales + critics scores/counts
fit <- lm(Global_Sales_log ~ Critic_Score, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL1 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                          data_frame(method="Critic Scores",
                                     SL = SL1))

fit <- lm(Global_Sales_log ~ Critic_Count, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL2 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="Critic Count",
                                   SL = SL2))


# fit linear regression model on global_sales + user scores/counts
fit <- lm(Global_Sales_log ~ User_Score, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL3 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="User Score",
                                   SL = SL3))

fit <- lm(Global_Sales_log ~ User_Count, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL4 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="User Count",
                                   SL = SL4))

SL_results %>% knitr::kable()
```

Of the individual scoring metrics the best predictor of global sales appears to be the number of critics who reviewed the game.

Next we consider linear models with multiple predictors.

```{r r linear individual, warning=FALSE, echo=FALSE}
#Multi-predictor algorithms
fit <- lm(Global_Sales_log ~ Critic_Score + User_Score, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL5 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="Critic + User Score",
                                   SL = SL5))

fit <- lm(Global_Sales_log ~ Critic_Score + Critic_Count, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL6 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="Critic Score + Count",
                                   SL = SL6))

fit <- lm(Global_Sales_log ~ Critic_Score + Critic_Count + User_Score + User_Count, data = train_critic)
fit$coefficients
y_hat <- predict(fit,test_critic)
SL7 <- mean((y_hat - test_critic$Global_Sales_log)^2)

SL_results <- bind_rows(SL_results,
                        data_frame(method="Critic Score + User Score + Critic Count + User Count",
                                   SL = SL7))

SL_results %>% knitr::kable()
```

The lowest mean squared loss is produced by the model using all 4 predictors.

We will now return to the previous algorithmic approach using the classification categories genre, platform, publisher and distributor to predictor critic and user reception by lowering the RMSE (rather than predicting global sales).

Firstly we will repeat the modelling to predict User Scores by incrementally adding classification categories.

```{r predict user scores, warning=FALSE, echo=FALSE}
# Predict user reception from other variables
# Model assuming same user score for all games
mu_hat <- mean(train_critic$User_Score)
mu_hat

naive_rmse <- RMSE(test_critic$User_Score, mu_hat)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

# Model based on on genre effect
mu <- mean(train_critic$User_Score) 
genre_avgs <- train_critic %>% 
  group_by(Genre) %>% 
  summarize(b_g = mean(User_Score - mu))

predicted_sales <- mu + test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  .$b_g

model_1_rmse <- RMSE (test_critic$User_Score,predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre Effect Model",
                                     RMSE = model_1_rmse))

# Model based on genre and platform effect
platform_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  group_by(Platform) %>%
  summarize(b_p = mean(User_Score - mu - b_g))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  mutate(pred = mu + b_g + b_p) %>%
  .$pred

model_2_rmse <- RMSE (test_critic$User_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform Effects Model",  
                                     RMSE = model_2_rmse ))

# Model based on genre, platform & publisher effect
publisher_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  group_by(Publisher) %>%
  summarize(b_u = mean(User_Score - mu - b_g - b_p))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  mutate(pred = mu + b_g + b_p + b_u) %>%
  .$pred

model_3_rmse <- RMSE (test_critic$User_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher Effects Model",  
                                     RMSE = model_3_rmse ))

# Model based on genre, platform, publisher & developer effect
developer_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  group_by(Developer) %>%
  summarize(b_d = mean(User_Score - mu - b_g - b_p - b_u))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  left_join(developer_avgs, by='Developer') %>%
  mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
  .$pred

model_4_rmse <- RMSE (test_critic$User_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher + Developer Effects Model",  
                                     RMSE = model_4_rmse ))

# Regularisation choosing penalty term for all effects
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_critic$User_Score)
  b_g <- train_critic %>%
    group_by(Genre) %>%
    summarize(b_g = sum(User_Score - mu)/(n()+l))
  b_p <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    group_by(Platform) %>%
    summarize(b_p = sum(User_Score - b_g - mu)/(n()+l))
  b_u <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    group_by(Publisher) %>%
    summarize(b_u = sum(User_Score - b_g - b_p - mu)/(n()+l))
  b_d <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>%
    group_by(Developer) %>%
    summarize(b_d = sum(User_Score - b_g - b_p - b_u - mu)/(n()+l))
  predicted_sales <- 
    test_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>% 
    left_join(b_d, by='Developer') %>% 
    mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
    .$pred
  return(RMSE(test_critic$User_Score, predicted_sales))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Genre + Platform + Publisher + Developer Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

The regularised model using all predictors and an optimal penalty of 2 returns an RMSE of 0.6198 which is a reasonable estimate of accuracy with overall average of 3.596.

Next we will repeat the algorithm modelling again, this time for Critic Scores.

```{r r predict user scores, warning=FALSE, echo=FALSE}

# Predict critical reception from other variables
# Model assuming same critic score for all games
mu_hat <- mean(train_critic$Critic_Score)
mu_hat

naive_rmse <- RMSE(test_critic$Critic_Score, mu_hat)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)

# Model based on on genre effect
mu <- mean(train_critic$Critic_Score) 
genre_avgs <- train_critic %>% 
  group_by(Genre) %>% 
  summarize(b_g = mean(Critic_Score - mu))

predicted_sales <- mu + test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  .$b_g

model_1_rmse <- RMSE (test_critic$Critic_Score,predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre Effect Model",
                                     RMSE = model_1_rmse))


# Model based on genre and platform effect
platform_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  group_by(Platform) %>%
  summarize(b_p = mean(Critic_Score - mu - b_g))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  mutate(pred = mu + b_g + b_p) %>%
  .$pred

model_2_rmse <- RMSE (test_critic$Critic_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform Effects Model",  
                                     RMSE = model_2_rmse ))

# Model based on genre, platform & publisher effect
publisher_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  group_by(Publisher) %>%
  summarize(b_u = mean(Critic_Score - mu - b_g - b_p))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  mutate(pred = mu + b_g + b_p + b_u) %>%
  .$pred

model_3_rmse <- RMSE (test_critic$Critic_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher Effects Model",  
                                     RMSE = model_3_rmse ))

# Model based on genre, platform, publisher & developer effect
developer_avgs <- train_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  group_by(Developer) %>%
  summarize(b_d = mean(Critic_Score - mu - b_g - b_p - b_u))

predicted_sales <- test_critic %>% 
  left_join(genre_avgs, by='Genre') %>%
  left_join(platform_avgs, by='Platform') %>%
  left_join(publisher_avgs, by='Publisher') %>%
  left_join(developer_avgs, by='Developer') %>%
  mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
  .$pred

model_4_rmse <- RMSE (test_critic$Critic_Score, predicted_sales)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Genre + Platform + Publisher + Developer Effects Model",  
                                     RMSE = model_4_rmse ))

# Regularisation choosing penalty term for all effects
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_critic$Critic_Score)
  b_g <- train_critic %>%
    group_by(Genre) %>%
    summarize(b_g = sum(Critic_Score - mu)/(n()+l))
  b_p <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    group_by(Platform) %>%
    summarize(b_p = sum(Critic_Score - b_g - mu)/(n()+l))
  b_u <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    group_by(Publisher) %>%
    summarize(b_u = sum(Critic_Score - b_g - b_p - mu)/(n()+l))
  b_d <- train_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>%
    group_by(Developer) %>%
    summarize(b_d = sum(Critic_Score - b_g - b_p - b_u - mu)/(n()+l))
  predicted_sales <- 
    test_critic %>% 
    left_join(b_g, by='Genre') %>%
    left_join(b_p, by='Platform') %>%
    left_join(b_u, by='Publisher') %>% 
    left_join(b_d, by='Developer') %>% 
    mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
    .$pred
  return(RMSE(test_critic$Critic_Score, predicted_sales))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Genre + Platform + Publisher + Developer Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()

```

The regularised model using all predictors and an optimal penalty of 1.25 returns an RMSE of 0.5495 against an overall average of 3.502 demonstrating even further improved accuracy relative to predicting user scores. This final model for predicting critic scores also shows greater improvement over a model only assuming just the average (down from 0.6738).

## Results

From the supporting analysis conducted the best predictive model identified is an aglorithmic approach using the classifications genre, platform, publisher and developer with an optimal penalty for low observations of 1.25 in order to predict critical reception (score).

The final RMSE of this model is:

```{r final model,warning=FALSE, echo=FALSE}
# Final Model
l = 1.25
mu <- mean(train_critic$Critic_Score)
b_g <- train_critic %>%
  group_by(Genre) %>%
  summarize(b_g = sum(Critic_Score - mu)/(n()+l))
b_p <- train_critic %>% 
  left_join(b_g, by='Genre') %>%
  group_by(Platform) %>%
  summarize(b_p = sum(Critic_Score - b_g - mu)/(n()+l))
b_u <- train_critic %>% 
  left_join(b_g, by='Genre') %>%
  left_join(b_p, by='Platform') %>%
  group_by(Publisher) %>%
  summarize(b_u = sum(Critic_Score - b_g - b_p - mu)/(n()+l))
b_d <- train_critic %>% 
  left_join(b_g, by='Genre') %>%
  left_join(b_p, by='Platform') %>%
  left_join(b_u, by='Publisher') %>%
  group_by(Developer) %>%
  summarize(b_d = sum(Critic_Score - b_g - b_p - b_u - mu)/(n()+l))
predicted_sales <- 
  test_critic %>% 
  left_join(b_g, by='Genre') %>%
  left_join(b_p, by='Platform') %>%
  left_join(b_u, by='Publisher') %>% 
  left_join(b_d, by='Developer') %>% 
  mutate(pred = mu + b_g + b_p + b_u + b_d) %>%
  .$pred

final_RMSE <- RMSE(test_critic$Critic_Score, predicted_sales)
final_RMSE
```

## Conclusion
Through the analysis conducted in this report I have considered a number of potential predictive models using video game sales and scoring data. Initially focusing on using classification categories to predict global sales I did not find a suitable model. In particular using Random Forest as a logistic predictive model proved highly inaccurate. 

Next I looked at the relationship between critic and user scoring to predict sales. These linear regression models provided a better fit than the previous algorithmic and logistic models to predict global sales.

Finally I reconsidered the algorithmic models but this time to predict user and critical reception. This analysis identified using genre, platform, publisher and developer to predict critic scores to be the most effective model.
